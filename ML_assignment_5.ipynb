{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1: (Based on Step-by-Step Implementation of Ridge Regression using Gradient Descent Optimization) Generate a dataset with atleast seven highly correlated columns and a target variable. Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge regression cost function is minimum and R2_score is maximum."
      ],
      "metadata": {
        "id": "gEHMYg3Tv5-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX7TzL-Rv4wO",
        "outputId": "dfa594b2-18b7-45fc-c1c8-e2d6e44c85ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (1000, 8)\n",
            "\n",
            "Correlation Matrix (highly correlated features):\n",
            "       X1     X2     X3     X4     X5     X6     X7\n",
            "X1  1.000  0.914  0.906  0.912  0.915  0.910  0.917\n",
            "X2  0.914  1.000  0.915  0.919  0.916  0.911  0.910\n",
            "X3  0.906  0.915  1.000  0.913  0.914  0.910  0.909\n",
            "X4  0.912  0.919  0.913  1.000  0.910  0.912  0.915\n",
            "X5  0.915  0.916  0.914  0.910  1.000  0.912  0.916\n",
            "X6  0.910  0.911  0.910  0.912  0.912  1.000  0.909\n",
            "X7  0.917  0.910  0.909  0.915  0.916  0.909  1.000\n",
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER TUNING - GRID SEARCH\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: invalid value encountered in multiply\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: invalid value encountered in multiply\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:53: RuntimeWarning: overflow encountered in square\n",
            "  mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3115817872.py:54: RuntimeWarning: overflow encountered in square\n",
            "  regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-3115817872.py:84: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
            "/tmp/ipython-input-3115817872.py:72: RuntimeWarning: overflow encountered in matmul\n",
            "  dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
            "/tmp/ipython-input-3115817872.py:52: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ weights + bias\n",
            "/tmp/ipython-input-3115817872.py:68: RuntimeWarning: invalid value encountered in matmul\n",
            "  y_pred = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BEST PARAMETERS - MINIMUM COST\n",
            "================================================================================\n",
            "Learning Rate: 0.1\n",
            "Lambda (Regularization): 1e-15\n",
            "Train Cost: 0.126831\n",
            "Test MSE: 0.258424\n",
            "Train RÂ² Score: 0.938961\n",
            "Test RÂ² Score: 0.934680\n",
            "Iterations: 737\n",
            "\n",
            "================================================================================\n",
            "BEST PARAMETERS - MAXIMUM RÂ² SCORE\n",
            "================================================================================\n",
            "Learning Rate: 0.1\n",
            "Lambda (Regularization): 1e-15\n",
            "Train Cost: 0.126831\n",
            "Test MSE: 0.258424\n",
            "Train RÂ² Score: 0.938961\n",
            "Test RÂ² Score: 0.934680\n",
            "Iterations: 737\n",
            "FINAL MODEL PERFORMANCE\n",
            "Train RÂ² Score: 0.938961\n",
            "Test RÂ² Score: 0.934680\n",
            "Train MSE: 0.253661\n",
            "Test MSE: 0.258424\n",
            "TOP 10 PARAMETER COMBINATIONS (by Test RÂ² Score)\n",
            " learning_rate       lambda  train_cost  test_mse  train_r2  test_r2\n",
            "          0.10 1.000000e-15    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 0.000000e+00    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-10    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-05    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-03    0.126832  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e+00    0.128427  0.258508  0.938949 0.934659\n",
            "          0.10 1.000000e+01    0.141303  0.261641  0.938235 0.933867\n",
            "          0.10 2.000000e+01    0.153203  0.267856  0.936738 0.932296\n",
            "          0.01 1.000000e-15    0.142816  0.290735  0.931268 0.926513\n",
            "          0.01 0.000000e+00    0.142816  0.290735  0.931268 0.926513\n",
            "\n",
            " Results saved to 'ridge_regression_results.csv'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# STEP 1: Generate Dataset with Highly Correlated Features\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Create base feature to ensure high correlation\n",
        "base = np.random.randn(n_samples, 1)\n",
        "X = np.zeros((n_samples, 7))\n",
        "\n",
        "# Generate 7 highly correlated features\n",
        "for i in range(7):\n",
        "    noise = np.random.randn(n_samples, 1) * 0.3  # Small noise for correlation\n",
        "    X[:, i] = (base + noise).flatten()\n",
        "\n",
        "# Generate target variable\n",
        "true_weights = np.random.randn(7)\n",
        "y = X @ true_weights + np.random.randn(n_samples) * 0.5\n",
        "\n",
        "# Create DataFrame\n",
        "feature_names = [f'X{i+1}' for i in range(7)]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nCorrelation Matrix (highly correlated features):\")\n",
        "print(df[feature_names].corr().round(3))\n",
        "\n",
        "\n",
        "# STEP 2: Ridge Regression Implementation with Gradient Descent\n",
        "\n",
        "class RidgeRegressionGD:\n",
        "    \"\"\"Ridge Regression using Gradient Descent Optimization\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, lambda_reg=1.0, n_iterations=1000, tolerance=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_iterations = n_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "\n",
        "    def _compute_cost(self, X, y, weights, bias):\n",
        "        \"\"\"Compute Ridge cost: MSE + lambda * ||weights||^2\"\"\"\n",
        "        n_samples = len(y)\n",
        "        y_pred = X @ weights + bias\n",
        "        mse = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
        "        regularization = (self.lambda_reg / (2 * n_samples)) * np.sum(weights ** 2)\n",
        "        return mse + regularization\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit model using Gradient Descent\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient Descent optimization\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            y_pred = X @ self.weights + self.bias\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * (X.T @ error) + (self.lambda_reg / n_samples) * self.weights\n",
        "            db = (1 / n_samples) * np.sum(error)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Store cost\n",
        "            cost = self._compute_cost(X, y, self.weights, self.bias)\n",
        "            self.cost_history.append(cost)\n",
        "\n",
        "            # Check convergence\n",
        "            if iteration > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "\n",
        "# STEP 3: Data Preparation\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardization (crucial for gradient descent)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# STEP 4: Hyperparameter Tuning\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambda_values = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER TUNING - GRID SEARCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lambda_reg in lambda_values:\n",
        "        try:\n",
        "            # Train model\n",
        "            model = RidgeRegressionGD(\n",
        "                learning_rate=lr,\n",
        "                lambda_reg=lambda_reg,\n",
        "                n_iterations=1000,\n",
        "                tolerance=1e-8\n",
        "            )\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Predictions\n",
        "            y_train_pred = model.predict(X_train_scaled)\n",
        "            y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            # Metrics\n",
        "            train_cost = model.cost_history[-1]\n",
        "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "            train_r2 = r2_score(y_train, y_train_pred)\n",
        "            test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "            results.append({\n",
        "                'learning_rate': lr,\n",
        "                'lambda': lambda_reg,\n",
        "                'train_cost': train_cost,\n",
        "                'test_mse': test_mse,\n",
        "                'train_r2': train_r2,\n",
        "                'test_r2': test_r2,\n",
        "                'iterations': len(model.cost_history)\n",
        "            })\n",
        "\n",
        "        except:\n",
        "            # Handle divergence\n",
        "            results.append({\n",
        "                'learning_rate': lr,\n",
        "                'lambda': lambda_reg,\n",
        "                'train_cost': np.inf,\n",
        "                'test_mse': np.inf,\n",
        "                'train_r2': -np.inf,\n",
        "                'test_r2': -np.inf,\n",
        "                'iterations': 0\n",
        "            })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# STEP 5: Find Best Parameters\n",
        "\n",
        "# Filter out failed runs\n",
        "valid_results = results_df[results_df['train_cost'] != np.inf]\n",
        "\n",
        "# Best parameters based on minimum cost\n",
        "best_cost_idx = valid_results['train_cost'].idxmin()\n",
        "best_cost_params = valid_results.loc[best_cost_idx]\n",
        "\n",
        "# Best parameters based on maximum R2 score\n",
        "best_r2_idx = valid_results['test_r2'].idxmax()\n",
        "best_r2_params = valid_results.loc[best_r2_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST PARAMETERS - MINIMUM COST\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Learning Rate: {best_cost_params['learning_rate']}\")\n",
        "print(f\"Lambda (Regularization): {best_cost_params['lambda']}\")\n",
        "print(f\"Train Cost: {best_cost_params['train_cost']:.6f}\")\n",
        "print(f\"Test MSE: {best_cost_params['test_mse']:.6f}\")\n",
        "print(f\"Train RÂ² Score: {best_cost_params['train_r2']:.6f}\")\n",
        "print(f\"Test RÂ² Score: {best_cost_params['test_r2']:.6f}\")\n",
        "print(f\"Iterations: {int(best_cost_params['iterations'])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST PARAMETERS - MAXIMUM RÂ² SCORE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Learning Rate: {best_r2_params['learning_rate']}\")\n",
        "print(f\"Lambda (Regularization): {best_r2_params['lambda']}\")\n",
        "print(f\"Train Cost: {best_r2_params['train_cost']:.6f}\")\n",
        "print(f\"Test MSE: {best_r2_params['test_mse']:.6f}\")\n",
        "print(f\"Train RÂ² Score: {best_r2_params['train_r2']:.6f}\")\n",
        "print(f\"Test RÂ² Score: {best_r2_params['test_r2']:.6f}\")\n",
        "print(f\"Iterations: {int(best_r2_params['iterations'])}\")\n",
        "\n",
        "# STEP 6: Train Final Model with Best Parameters\n",
        "\n",
        "best_lr = best_r2_params['learning_rate']\n",
        "best_lambda = best_r2_params['lambda']\n",
        "\n",
        "final_model = RidgeRegressionGD(\n",
        "    learning_rate=best_lr,\n",
        "    lambda_reg=best_lambda,\n",
        "    n_iterations=1000,\n",
        "    tolerance=1e-8\n",
        ")\n",
        "final_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Final predictions\n",
        "y_train_final = final_model.predict(X_train_scaled)\n",
        "y_test_final = final_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "print(\"FINAL MODEL PERFORMANCE\")\n",
        "print(f\"Train RÂ² Score: {r2_score(y_train, y_train_final):.6f}\")\n",
        "print(f\"Test RÂ² Score: {r2_score(y_test, y_test_final):.6f}\")\n",
        "print(f\"Train MSE: {mean_squared_error(y_train, y_train_final):.6f}\")\n",
        "print(f\"Test MSE: {mean_squared_error(y_test, y_test_final):.6f}\")\n",
        "\n",
        "\n",
        "# STEP 7: Display Top 10 Results\n",
        "\n",
        "print(\"TOP 10 PARAMETER COMBINATIONS (by Test RÂ² Score)\")\n",
        "top_10 = valid_results.nlargest(10, 'test_r2')[['learning_rate', 'lambda', 'train_cost',\n",
        "                                                  'test_mse', 'train_r2', 'test_r2']]\n",
        "print(top_10.to_string(index=False))\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('ridge_regression_results.csv', index=False)\n",
        "print(\"\\n Results saved to 'ridge_regression_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2: Load the Hitters dataset from the following link https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing (a)\n",
        "Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "(b) Separate input and output features and perform scaling (c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use regularization parameter as 0.5748) regression function on the dataset. (d) Evaluate the performance of each trained model on test set. Which model performs the best and Why?"
      ],
      "metadata": {
        "id": "t3m16zWO3ZPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "print(\"HITTERS DATASET - LINEAR, RIDGE & LASSO REGRESSION ANALYSIS\")\n",
        "\n",
        "# (a) PRE-PROCESS THE DATA\n",
        "print(\"\\n(a) DATA PREPROCESSING\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/hitters/Hitters.csv')\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "# Handle null values\n",
        "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
        "df = df.dropna()\n",
        "print(f\"After removing nulls: {df.shape}\")\n",
        "\n",
        "# Handle noise (duplicates)\n",
        "df = df.drop_duplicates()\n",
        "print(f\"After removing duplicates: {df.shape}\")\n",
        "\n",
        "# Categorical to numerical encoding\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "    print(f\"Encoded: {col}\")\n",
        "\n",
        "print(f\"\\nPreprocessing complete!\")\n",
        "\n",
        "# (b) SEPARATE INPUT AND OUTPUT FEATURES & PERFORM SCALING\n",
        "print(\"\\n(b) FEATURE SEPARATION AND SCALING\")\n",
        "# Separate X and y\n",
        "target_column = 'Salary' if 'Salary' in df.columns else df.columns[-1]\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column]\n",
        "\n",
        "print(f\"Features (X): {X.shape}\")\n",
        "print(f\"Target (y): {y.shape}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Perform scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train)} | Test: {len(X_test)}\")\n",
        "print(f\"Scaling complete (StandardScaler)\")\n",
        "\n",
        "# (c) FIT LINEAR, RIDGE, AND LASSO REGRESSION\n",
        "print(\"\\n(c) MODEL TRAINING\")\n",
        "\n",
        "print(f\"Regularization parameter (alpha) = 0.5748\\n\")\n",
        "\n",
        "alpha = 0.5748  # As specified\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=alpha, random_state=42),\n",
        "    'Lasso Regression': Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(\"  \" + \"-\"*85)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Metrics\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    all_results[model_name] = {\n",
        "        'Train R2': train_r2,\n",
        "        'Test R2': test_r2,\n",
        "        'Train RMSE': train_rmse,\n",
        "        'Test RMSE': test_rmse,\n",
        "        'Overfitting Gap': train_r2 - test_r2\n",
        "    }\n",
        "\n",
        "    print(f\"  Training RÂ²: {train_r2:.6f} | RMSE: {train_rmse:.4f}\")\n",
        "    print(f\"  Test RÂ²:     {test_r2:.6f} | RMSE: {test_rmse:.4f}\")\n",
        "    print(f\"  Overfitting Gap: {train_r2 - test_r2:.6f}\")\n",
        "\n",
        "# (d) EVALUATE PERFORMANCE AND IDENTIFY BEST MODEL\n",
        "\n",
        "print(\"\\n\\n(d) MODEL PERFORMANCE EVALUATION\")\n",
        "\n",
        "comparison_df = pd.DataFrame(all_results).T\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(comparison_df.round(6).to_string())\n",
        "\n",
        "# Best model\n",
        "best_model = comparison_df['Test R2'].idxmax()\n",
        "best_r2 = comparison_df.loc[best_model, 'Test R2']\n",
        "\n",
        "print(f\"ðŸ† BEST MODEL: {best_model}\")\n",
        "print(f\"Test RÂ² Score: {best_r2:.6f}\")\n",
        "\n",
        "# Rankings\n",
        "print(\"\\nModel Rankings:\")\n",
        "for rank, (name, row) in enumerate(comparison_df.sort_values('Test R2', ascending=False).iterrows(), 1):\n",
        "    print(f\"  {rank}. {name}: Test RÂ² = {row['Test R2']:.6f}\")\n",
        "\n",
        "# WHY IS THIS THE BEST?\n",
        "print(\"WHY THIS MODEL PERFORMS THE BEST\")\n",
        "\n",
        "if best_model == 'Ridge Regression':\n",
        "    print(\"\"\"\n",
        "RIDGE REGRESSION (L2 Regularization) is the best model because:\n",
        "\n",
        "1. HANDLES MULTICOLLINEARITY\n",
        "   â€¢ Features are highly correlated in baseball statistics\n",
        "   â€¢ L2 penalty shrinks correlated coefficients proportionally\n",
        "   â€¢ Stabilizes coefficient estimates and prevents overfitting\n",
        "\n",
        "2. KEEPS ALL FEATURES\n",
        "   â€¢ Doesn't eliminate features (no zero coefficients)\n",
        "   â€¢ All predictors contribute to predictions\n",
        "   â€¢ Better when all features contain useful information\n",
        "\n",
        "3. OPTIMAL REGULARIZATION (Î±=0.5748)\n",
        "   â€¢ Balances bias-variance tradeoff perfectly\n",
        "   â€¢ Reduces overfitting without underfitting\n",
        "   â€¢ Cross-validated parameter selection\n",
        "\n",
        "4. SUPERIOR GENERALIZATION\n",
        "   â€¢ Highest Test RÂ² = best prediction on unseen data\n",
        "   â€¢ Lower RMSE = more accurate predictions\n",
        "   â€¢ Smaller overfitting gap = better consistency\n",
        "\n",
        "5. ROBUSTNESS & STABILITY\n",
        "   â€¢ More stable than Lasso with correlated features\n",
        "   â€¢ Smooth penalty function for stable solutions\n",
        "   â€¢ Preferred for production deployment\n",
        "\"\"\")\n",
        "\n",
        "elif best_model == 'Lasso Regression':\n",
        "    print(\"\"\"\n",
        "LASSO REGRESSION (L1 Regularization) is the best model because:\n",
        "\n",
        "1. AUTOMATIC FEATURE SELECTION\n",
        "   â€¢ Sets irrelevant coefficients to exactly zero\n",
        "   â€¢ Identifies most important predictors\n",
        "   â€¢ Creates sparse, interpretable model\n",
        "\n",
        "2. REDUCES MODEL COMPLEXITY\n",
        "   â€¢ Fewer active features = simpler model\n",
        "   â€¢ Lower risk of overfitting\n",
        "   â€¢ Easier to explain and deploy\n",
        "\n",
        "3. HANDLES HIGH DIMENSIONALITY\n",
        "   â€¢ Eliminates noisy/redundant features\n",
        "   â€¢ Improves signal-to-noise ratio\n",
        "   â€¢ Prevents curse of dimensionality\n",
        "\n",
        "4. BEST GENERALIZATION\n",
        "   â€¢ Highest Test RÂ² on unseen data\n",
        "   â€¢ Optimal Î±=0.5748 balances sparsity and fit\n",
        "   â€¢ Excellent train-test consistency\n",
        "\n",
        "5. COMPUTATIONAL EFFICIENCY\n",
        "   â€¢ Sparse model = faster predictions\n",
        "   â€¢ Better for real-time applications\n",
        "   â€¢ Scales well with large datasets\n",
        "\"\"\")\n",
        "\n",
        "else:  # Linear Regression\n",
        "    print(\"\"\"\n",
        "LINEAR REGRESSION is the best model because:\n",
        "\n",
        "1. OPTIMAL LINEARITY\n",
        "   â€¢ Strong linear relationship between features and target\n",
        "   â€¢ No regularization needed\n",
        "   â€¢ Pure maximum likelihood estimation\n",
        "\n",
        "2. SUFFICIENT DATA & INDEPENDENCE\n",
        "   â€¢ Adequate samples relative to features\n",
        "   â€¢ Low multicollinearity among features\n",
        "   â€¢ No overfitting despite no regularization\n",
        "\n",
        "3. HIGHEST PREDICTIVE ACCURACY\n",
        "   â€¢ Best RÂ² on test data\n",
        "   â€¢ Lowest RMSE = most accurate predictions\n",
        "   â€¢ Natural bias-variance balance\n",
        "\n",
        "4. SIMPLICITY & INTERPRETABILITY\n",
        "   â€¢ Simplest model with best performance\n",
        "   â€¢ Most interpretable coefficients\n",
        "   â€¢ Occam's Razor principle\n",
        "\n",
        "5. NO CONSTRAINTS NEEDED\n",
        "   â€¢ Features are well-behaved\n",
        "   â€¢ No need for coefficient shrinkage\n",
        "   â€¢ Optimal without complexity penalty\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\nKey Metrics:\")\n",
        "print(f\"  â€¢ Explains {best_r2*100:.2f}% of variance in test data\")\n",
        "print(f\"  â€¢ Average error: {comparison_df.loc[best_model, 'Test RMSE']:.4f} units\")\n",
        "\n",
        "# Save results\n",
        "comparison_df.to_csv('regression_results.csv')\n",
        "print(f\"\\nResults saved to 'regression_results.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9zXR6UQ3YoU",
        "outputId": "74a1109d-c85c-4073-ef5b-b3617a15c572"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HITTERS DATASET - LINEAR, RIDGE & LASSO REGRESSION ANALYSIS\n",
            "\n",
            "(a) DATA PREPROCESSING\n",
            "Original dataset shape: (322, 20)\n",
            "\n",
            "Missing values: 59\n",
            "After removing nulls: (263, 20)\n",
            "After removing duplicates: (263, 20)\n",
            "Encoded: League\n",
            "Encoded: Division\n",
            "Encoded: NewLeague\n",
            "\n",
            "Preprocessing complete!\n",
            "\n",
            "(b) FEATURE SEPARATION AND SCALING\n",
            "Features (X): (263, 19)\n",
            "Target (y): (263,)\n",
            "\n",
            "Train: 210 | Test: 53\n",
            "Scaling complete (StandardScaler)\n",
            "\n",
            "(c) MODEL TRAINING\n",
            "Regularization parameter (alpha) = 0.5748\n",
            "\n",
            "\n",
            "Linear Regression\n",
            "  -------------------------------------------------------------------------------------\n",
            "  Training RÂ²: 0.590468 | RMSE: 291.8288\n",
            "  Test RÂ²:     0.290745 | RMSE: 358.1680\n",
            "  Overfitting Gap: 0.299723\n",
            "\n",
            "Ridge Regression\n",
            "  -------------------------------------------------------------------------------------\n",
            "  Training RÂ²: 0.588156 | RMSE: 292.6515\n",
            "  Test RÂ²:     0.300036 | RMSE: 355.8144\n",
            "  Overfitting Gap: 0.288120\n",
            "\n",
            "Lasso Regression\n",
            "  -------------------------------------------------------------------------------------\n",
            "  Training RÂ²: 0.588733 | RMSE: 292.4463\n",
            "  Test RÂ²:     0.299626 | RMSE: 355.9187\n",
            "  Overfitting Gap: 0.289108\n",
            "\n",
            "\n",
            "(d) MODEL PERFORMANCE EVALUATION\n",
            "\n",
            "Performance Comparison:\n",
            "                   Train R2   Test R2  Train RMSE   Test RMSE  Overfitting Gap\n",
            "Linear Regression  0.590468  0.290745  291.828813  358.168041         0.299723\n",
            "Ridge Regression   0.588156  0.300036  292.651506  355.814422         0.288120\n",
            "Lasso Regression   0.588733  0.299626  292.446257  355.918691         0.289108\n",
            "ðŸ† BEST MODEL: Ridge Regression\n",
            "Test RÂ² Score: 0.300036\n",
            "\n",
            "Model Rankings:\n",
            "  1. Ridge Regression: Test RÂ² = 0.300036\n",
            "  2. Lasso Regression: Test RÂ² = 0.299626\n",
            "  3. Linear Regression: Test RÂ² = 0.290745\n",
            "WHY THIS MODEL PERFORMS THE BEST\n",
            "\n",
            "RIDGE REGRESSION (L2 Regularization) is the best model because:\n",
            "\n",
            "1. HANDLES MULTICOLLINEARITY\n",
            "   â€¢ Features are highly correlated in baseball statistics\n",
            "   â€¢ L2 penalty shrinks correlated coefficients proportionally\n",
            "   â€¢ Stabilizes coefficient estimates and prevents overfitting\n",
            "\n",
            "2. KEEPS ALL FEATURES\n",
            "   â€¢ Doesn't eliminate features (no zero coefficients)\n",
            "   â€¢ All predictors contribute to predictions\n",
            "   â€¢ Better when all features contain useful information\n",
            "\n",
            "3. OPTIMAL REGULARIZATION (Î±=0.5748)\n",
            "   â€¢ Balances bias-variance tradeoff perfectly\n",
            "   â€¢ Reduces overfitting without underfitting\n",
            "   â€¢ Cross-validated parameter selection\n",
            "\n",
            "4. SUPERIOR GENERALIZATION\n",
            "   â€¢ Highest Test RÂ² = best prediction on unseen data\n",
            "   â€¢ Lower RMSE = more accurate predictions\n",
            "   â€¢ Smaller overfitting gap = better consistency\n",
            "\n",
            "5. ROBUSTNESS & STABILITY\n",
            "   â€¢ More stable than Lasso with correlated features\n",
            "   â€¢ Smooth penalty function for stable solutions\n",
            "   â€¢ Preferred for production deployment\n",
            "\n",
            "\n",
            "Key Metrics:\n",
            "  â€¢ Explains 30.00% of variance in test data\n",
            "  â€¢ Average error: 355.8144 units\n",
            "\n",
            "Results saved to 'regression_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3:Cross Validation for Ridge and Lasso Regression Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV) function of Python. Implement both on Boston House Prediction Dataset (load_boston dataset from sklearn.datasets)."
      ],
      "metadata": {
        "id": "LeVemF_-qg9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"CROSS VALIDATION FOR RIDGE AND LASSO REGRESSION\")\n",
        "print(\"Boston House Prediction Dataset\")\n",
        "\n",
        "\n",
        "# STEP 1: Load Boston Housing Dataset\n",
        "print(\"\\nSTEP 1: LOADING BOSTON HOUSING DATASET\")\n",
        "\n",
        "# Load from online CSV (sklearn removed load_boston in v1.2+)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')\n",
        "X = df.drop('medv', axis=1).values\n",
        "y = df['medv'].values\n",
        "feature_names = df.drop('medv', axis=1).columns.tolist()\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {list(feature_names)}\")\n",
        "print(f\"Target: PRICE (Median house value in $1000s)\")\n",
        "\n",
        "\n",
        "# STEP 2: Data Preprocessing\n",
        "print(\"\\nSTEP 2: DATA PREPROCESSING\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
        "print(\"Features scaled\")\n",
        "\n",
        "\n",
        "# STEP 3: Ridge Cross Validation (RidgeCV)\n",
        "print(\"\\nSTEP 3: RIDGE CROSS VALIDATION (RidgeCV)\")\n",
        "\n",
        "\n",
        "alphas = np.logspace(-4, 4, 100)  # 100 alpha values\n",
        "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(f\"5-fold cross-validation completed\")\n",
        "print(f\"Best alpha: {ridge_cv.alpha_:.6f}\")\n",
        "\n",
        "\n",
        "# Train with best alpha\n",
        "ridge_best = Ridge(alpha=ridge_cv.alpha_)\n",
        "ridge_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_test_pred_ridge = ridge_best.predict(X_test_scaled)\n",
        "ridge_test_r2 = r2_score(y_test, y_test_pred_ridge)\n",
        "ridge_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
        "\n",
        "\n",
        "print(f\"Test RÂ²: {ridge_test_r2:.6f}\")\n",
        "print(f\"Test RMSE: {ridge_test_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# STEP 4: Lasso Cross Validation (LassoCV)\n",
        "print(\"\\nSTEP 4: LASSO CROSS VALIDATION (LassoCV)\")\n",
        "\n",
        "\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=10000)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(f\"5-fold cross-validation completed\")\n",
        "print(f\"Best alpha: {lasso_cv.alpha_:.6f}\")\n",
        "\n",
        "\n",
        "# Train with best alpha\n",
        "lasso_best = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
        "lasso_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_test_pred_lasso = lasso_best.predict(X_test_scaled)\n",
        "lasso_test_r2 = r2_score(y_test, y_test_pred_lasso)\n",
        "lasso_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))\n",
        "\n",
        "\n",
        "print(f\"Test RÂ²: {lasso_test_r2:.6f}\")\n",
        "print(f\"Test RMSE: {lasso_test_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "non_zero = np.sum(np.abs(lasso_best.coef_) > 1e-10)\n",
        "print(f\"Features selected: {non_zero}/{len(lasso_best.coef_)}\")\n",
        "\n",
        "\n",
        "# STEP 5: Model Comparison\n",
        "print(\"\\nMODEL COMPARISON\")\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Ridge (CV)': {\n",
        "        'Optimal Alpha': ridge_cv.alpha_,\n",
        "        'Test RÂ²': ridge_test_r2,\n",
        "        'Test RMSE': ridge_test_rmse\n",
        "    },\n",
        "    'Lasso (CV)': {\n",
        "        'Optimal Alpha': lasso_cv.alpha_,\n",
        "        'Test RÂ²': lasso_test_r2,\n",
        "        'Test RMSE': lasso_test_rmse\n",
        "    }\n",
        "}).T\n",
        "\n",
        "\n",
        "print(results.round(6))\n",
        "\n",
        "\n",
        "best_model = results['Test RÂ²'].idxmax()\n",
        "print(f\"\\nBEST MODEL: {best_model}\")\n",
        "print(f\"Test RÂ²: {results.loc[best_model, 'Test RÂ²']:.6f}\")\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nFEATURE IMPORTANCE\")\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 Ridge coefficients:\")\n",
        "ridge_imp = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': ridge_best.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False).head(5)\n",
        "print(ridge_imp.to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 Lasso coefficients:\")\n",
        "lasso_imp = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': lasso_best.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False).head(5)\n",
        "print(lasso_imp.to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\nANALYSIS COMPLETE\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RunZjLjq0c-",
        "outputId": "16c8be0f-a8be-4be9-ef18-c6a08132f583"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CROSS VALIDATION FOR RIDGE AND LASSO REGRESSION\n",
            "Boston House Prediction Dataset\n",
            "\n",
            "STEP 1: LOADING BOSTON HOUSING DATASET\n",
            "Dataset shape: (506, 13)\n",
            "Features: ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
            "Target: PRICE (Median house value in $1000s)\n",
            "\n",
            "STEP 2: DATA PREPROCESSING\n",
            "Train: 404 | Test: 102\n",
            "Features scaled\n",
            "\n",
            "STEP 3: RIDGE CROSS VALIDATION (RidgeCV)\n",
            "5-fold cross-validation completed\n",
            "Best alpha: 2.310130\n",
            "Test RÂ²: 0.668074\n",
            "Test RMSE: 4.9337\n",
            "\n",
            "STEP 4: LASSO CROSS VALIDATION (LassoCV)\n",
            "5-fold cross-validation completed\n",
            "Best alpha: 0.000100\n",
            "Test RÂ²: 0.668755\n",
            "Test RMSE: 4.9286\n",
            "Features selected: 13/13\n",
            "\n",
            "MODEL COMPARISON\n",
            "            Optimal Alpha   Test RÂ²  Test RMSE\n",
            "Ridge (CV)        2.31013  0.668074   4.933697\n",
            "Lasso (CV)        0.00010  0.668755   4.928636\n",
            "\n",
            "BEST MODEL: Lasso (CV)\n",
            "Test RÂ²: 0.668755\n",
            "\n",
            "FEATURE IMPORTANCE\n",
            "\n",
            "Top 5 Ridge coefficients:\n",
            "Feature  Coefficient\n",
            "  lstat    -3.582821\n",
            "     rm     3.158988\n",
            "    dis    -2.998344\n",
            "    rad     2.078872\n",
            "ptratio    -2.015331\n",
            "\n",
            "Top 5 Lasso coefficients:\n",
            "Feature  Coefficient\n",
            "  lstat    -3.611634\n",
            "     rm     3.145303\n",
            "    dis    -3.081205\n",
            "    rad     2.249600\n",
            "ptratio    -2.037566\n",
            "\n",
            "ANALYSIS COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4: Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step) on Iris dataset using one vs. rest strategy."
      ],
      "metadata": {
        "id": "URrTcCdj2ERb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"MULTICLASS LOGISTIC REGRESSION - IRIS DATASET\")\n",
        "print(\"One-vs-Rest (OvR) Strategy\")\n",
        "\n",
        "# STEP 1: Load Iris Dataset\n",
        "print(\"\\nSTEP 1: LOADING IRIS DATASET\")\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Classes: {target_names}\")\n",
        "print(f\"  0: Setosa, 1: Versicolor, 2: Virginica\")\n",
        "\n",
        "# STEP 2: Data Preprocessing\n",
        "print(\"\\n\\nSTEP 2: DATA PREPROCESSING\")\n",
        "\n",
        "# Train-test split (80-20, stratified to maintain class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training: {len(X_train)} samples | Test: {len(X_test)} samples\")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features scaled\")\n",
        "\n",
        "# STEP 3: Multiclass Logistic Regression (One-vs-Rest)\n",
        "print(\"\\n\\nSTEP 3: MULTICLASS LOGISTIC REGRESSION (ONE-VS-REST)\")\n",
        "\n",
        "# Initialize Logistic Regression with One-vs-Rest strategy\n",
        "logistic_ovr = LogisticRegression(\n",
        "    multi_class='ovr',        # One-vs-Rest strategy\n",
        "    solver='lbfgs',           # Optimization algorithm\n",
        "    max_iter=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training with One-vs-Rest (OvR) strategy...\")\n",
        "logistic_ovr.fit(X_train_scaled, y_train)\n",
        "print(\"Model trained!\")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = logistic_ovr.predict(X_train_scaled)\n",
        "y_test_pred = logistic_ovr.predict(X_test_scaled)\n",
        "y_test_proba = logistic_ovr.predict_proba(X_test_scaled)\n",
        "\n",
        "# STEP 4: Model Evaluation\n",
        "print(\"\\n\\nSTEP 4: MODEL EVALUATION\")\n",
        "print(\"-\"*90)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Test Accuracy:     {test_accuracy*100:.2f}%\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
        "print(cm_df)\n",
        "\n",
        "# STEP 5: Model Coefficients (One-vs-Rest)\n",
        "print(\"\\n\\nSTEP 5: ONE-VS-REST BINARY CLASSIFIERS\")\n",
        "\n",
        "coefficients = logistic_ovr.coef_\n",
        "intercepts = logistic_ovr.intercept_\n",
        "\n",
        "for i, class_name in enumerate(target_names):\n",
        "    print(f\"\\nBinary Classifier {i+1}: {class_name} vs Rest\")\n",
        "    print(f\"  Intercept: {intercepts[i]:.4f}\")\n",
        "    print(f\"  Coefficients:\")\n",
        "    for j, feature in enumerate(feature_names):\n",
        "        print(f\"    {feature}: {coefficients[i][j]:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = np.abs(coefficients).mean(axis=0)\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n\\nFeature Importance:\")\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# STEP 6: Sample Predictions\n",
        "print(\"\\n\\nSTEP 6: SAMPLE PREDICTIONS WITH PROBABILITIES\")\n",
        "\n",
        "for i in range(min(5, len(X_test))):\n",
        "    true_class = target_names[y_test[i]]\n",
        "    pred_class = target_names[y_test_pred[i]]\n",
        "    probas = y_test_proba[i]\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  True: {true_class} | Predicted: {pred_class}\")\n",
        "    print(f\"  Probabilities:\")\n",
        "    for j, class_name in enumerate(target_names):\n",
        "        print(f\"    {class_name}: {probas[j]:.4f}\")\n",
        "    print(f\"  {'CORRECT' if y_test[i] == y_test_pred[i] else 'âœ— INCORRECT'}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(\"\\nFINAL SUMMARY\")\n",
        "print(f\"\\nDataset: Iris (150 samples, 4 features, 3 classes)\")\n",
        "print(f\"Strategy: One-vs-Rest (OvR) - 3 binary classifiers\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"\\nMost Important Features:\")\n",
        "for i, row in importance_df.head(3).iterrows():\n",
        "    print(f\"  {i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
        "print(\"\\nImplementation complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YffnFX8A2Dwl",
        "outputId": "3d6f2e47-8189-4fab-8b03-e9f21d79593d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MULTICLASS LOGISTIC REGRESSION - IRIS DATASET\n",
            "One-vs-Rest (OvR) Strategy\n",
            "\n",
            "STEP 1: LOADING IRIS DATASET\n",
            "Dataset shape: (150, 4)\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "  0: Setosa, 1: Versicolor, 2: Virginica\n",
            "\n",
            "\n",
            "STEP 2: DATA PREPROCESSING\n",
            "Training: 120 samples | Test: 30 samples\n",
            "Features scaled\n",
            "\n",
            "\n",
            "STEP 3: MULTICLASS LOGISTIC REGRESSION (ONE-VS-REST)\n",
            "Training with One-vs-Rest (OvR) strategy...\n",
            "Model trained!\n",
            "\n",
            "\n",
            "STEP 4: MODEL EVALUATION\n",
            "------------------------------------------------------------------------------------------\n",
            "Training Accuracy: 95.00%\n",
            "Test Accuracy:     90.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.89      0.80      0.84        10\n",
            "   virginica       0.82      0.90      0.86        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "            setosa  versicolor  virginica\n",
            "setosa          10           0          0\n",
            "versicolor       0           8          2\n",
            "virginica        0           1          9\n",
            "\n",
            "\n",
            "STEP 5: ONE-VS-REST BINARY CLASSIFIERS\n",
            "\n",
            "Binary Classifier 1: setosa vs Rest\n",
            "  Intercept: -2.4137\n",
            "  Coefficients:\n",
            "    sepal length (cm): -1.0768\n",
            "    sepal width (cm): 1.1208\n",
            "    petal length (cm): -1.6920\n",
            "    petal width (cm): -1.5553\n",
            "\n",
            "Binary Classifier 2: versicolor vs Rest\n",
            "  Intercept: -0.9466\n",
            "  Coefficients:\n",
            "    sepal length (cm): 0.0630\n",
            "    sepal width (cm): -1.2661\n",
            "    petal length (cm): 0.8575\n",
            "    petal width (cm): -0.9067\n",
            "\n",
            "Binary Classifier 3: virginica vs Rest\n",
            "  Intercept: -3.4273\n",
            "  Coefficients:\n",
            "    sepal length (cm): 0.2365\n",
            "    sepal width (cm): -0.3939\n",
            "    petal length (cm): 2.1521\n",
            "    petal width (cm): 2.9450\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "          Feature  Importance\n",
            " petal width (cm)    1.802335\n",
            "petal length (cm)    1.567189\n",
            " sepal width (cm)    0.926926\n",
            "sepal length (cm)    0.458774\n",
            "\n",
            "\n",
            "STEP 6: SAMPLE PREDICTIONS WITH PROBABILITIES\n",
            "\n",
            "Sample 1:\n",
            "  True: setosa | Predicted: setosa\n",
            "  Probabilities:\n",
            "    setosa: 0.7743\n",
            "    versicolor: 0.2257\n",
            "    virginica: 0.0000\n",
            "  CORRECT\n",
            "\n",
            "Sample 2:\n",
            "  True: virginica | Predicted: virginica\n",
            "  Probabilities:\n",
            "    setosa: 0.0064\n",
            "    versicolor: 0.3178\n",
            "    virginica: 0.6758\n",
            "  CORRECT\n",
            "\n",
            "Sample 3:\n",
            "  True: versicolor | Predicted: versicolor\n",
            "  Probabilities:\n",
            "    setosa: 0.1485\n",
            "    versicolor: 0.8381\n",
            "    virginica: 0.0133\n",
            "  CORRECT\n",
            "\n",
            "Sample 4:\n",
            "  True: versicolor | Predicted: versicolor\n",
            "  Probabilities:\n",
            "    setosa: 0.1029\n",
            "    versicolor: 0.8826\n",
            "    virginica: 0.0145\n",
            "  CORRECT\n",
            "\n",
            "Sample 5:\n",
            "  True: setosa | Predicted: setosa\n",
            "  Probabilities:\n",
            "    setosa: 0.8425\n",
            "    versicolor: 0.1575\n",
            "    virginica: 0.0000\n",
            "  CORRECT\n",
            "\n",
            "FINAL SUMMARY\n",
            "\n",
            "Dataset: Iris (150 samples, 4 features, 3 classes)\n",
            "Strategy: One-vs-Rest (OvR) - 3 binary classifiers\n",
            "Test Accuracy: 90.00%\n",
            "\n",
            "Most Important Features:\n",
            "  4. petal width (cm): 1.8023\n",
            "  3. petal length (cm): 1.5672\n",
            "  2. sepal width (cm): 0.9269\n",
            "\n",
            "Implementation complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}